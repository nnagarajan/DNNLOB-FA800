{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:19:09.954256Z",
     "start_time": "2025-11-28T02:19:09.951812Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:56:31.297000Z",
     "iopub.status.busy": "2025-11-28T04:56:31.296160Z",
     "iopub.status.idle": "2025-11-28T04:56:35.235145Z",
     "shell.execute_reply": "2025-11-28T04:56:35.231221Z",
     "shell.execute_reply.started": "2025-11-28T04:56:31.296938Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load packages\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)\n",
    "# N, D = X_train.shape\n",
    "from datetime import date\n",
    "import logging as log\n",
    "from models.dataset import Dataset as Dataset\n",
    "\n",
    "from models.utils import add_horizons, normalize\n",
    "from models.gd import GradientDescent as GradientDescent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b26b79b5b1e8a110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:17:21.788291Z",
     "start_time": "2025-11-28T02:17:21.786487Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:57:59.915434Z",
     "iopub.status.busy": "2025-11-28T04:57:59.914572Z",
     "iopub.status.idle": "2025-11-28T04:57:59.928623Z",
     "shell.execute_reply": "2025-11-28T04:57:59.925535Z",
     "shell.execute_reply.started": "2025-11-28T04:57:59.915380Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "tickers=\"slv,soxs,gdx,spy\"\n",
    "#tickers=\"slv,\"\n",
    "#tickers=(\"slv\",)\n",
    "iteration_no=6\n",
    "model_shortname=\"universal\"\n",
    "#0-> 10, 1->50,2->100\n",
    "horizon_mapping = {\n",
    "    0: 10,\n",
    "    1: 50,\n",
    "    2: 100\n",
    "}\n",
    "horizon_to_predict=1\n",
    "look_back_window=100\n",
    "batch_size=2048\n",
    "alpha=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f4bb19e5d9368b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:17:25.251243Z",
     "start_time": "2025-11-28T02:17:25.249774Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:58:00.370984Z",
     "iopub.status.busy": "2025-11-28T04:58:00.370102Z",
     "iopub.status.idle": "2025-11-28T04:58:00.382396Z",
     "shell.execute_reply": "2025-11-28T04:58:00.379193Z",
     "shell.execute_reply.started": "2025-11-28T04:58:00.370928Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint_loc=f\"/results/iteration{iteration_no}\"\n",
    "os.makedirs(f\".{model_checkpoint_loc}\", exist_ok=True)\n",
    "tickers = tuple(x.strip() for x in tickers.split(\",\"))\n",
    "print(f\"Running model with params Ticker: {tickers} Horizon Idx:{horizon_to_predict} window:{look_back_window} training batch size:{batch_size} alpha {alpha} iteration {iteration_no}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039254e-6772-48e5-b1ea-8d4efb925853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure basic logging to a file\n",
    "log.basicConfig(\n",
    "    filename=f\"{model_checkpoint_loc}/experiments.log\",  # Name of the log file\n",
    "    level=log.INFO,             # Minimum logging level to capture (e.g., INFO, DEBUG, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format of the log messages\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'      # Format for the timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61849c91-7981-40fe-a8f8-60f4a6d0270b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:17:25.251243Z",
     "start_time": "2025-11-28T02:17:25.249774Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:58:00.370984Z",
     "iopub.status.busy": "2025-11-28T04:58:00.370102Z",
     "iopub.status.idle": "2025-11-28T04:58:00.382396Z",
     "shell.execute_reply": "2025-11-28T04:58:00.379193Z",
     "shell.execute_reply.started": "2025-11-28T04:58:00.370928Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path=\"data/\"\n",
    "prefix=\"_cleaned_jan2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc3069812eee63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:19:52.531388Z",
     "start_time": "2025-11-28T02:19:32.048113Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    log.info(f\"Processing {ticker}\")\n",
    "    print(f\"Processing {ticker}\")\n",
    "    dataset_train: Optional[Dataset] = None\n",
    "    dataset_val: Optional[Dataset] = None\n",
    "    dataset_test: Optional[Dataset] = None\n",
    "    df = pd.read_csv(f\"{data_path}{ticker}{prefix}.csv\",engine=\"pyarrow\",sep = ',')\n",
    "    df.head()\n",
    "    df[\"Date-Time\"] = pd.to_datetime(df[\"Date-Time\"])\n",
    "    df[\"Date-Time\"] = df[\"Date-Time\"].dt.tz_convert(\"America/New_York\")\n",
    "    add_horizons(df,(10, 50, 100),alpha)\n",
    "    class_summary = df.groupby(f\"Target_{horizon_mapping[horizon_to_predict]}\").size().reset_index(name=\"Count\")\n",
    "    # Calculate relative percentage\n",
    "    class_summary[\"Percent\"] = (class_summary[\"Count\"] / class_summary[\"Count\"].sum()) * 100\n",
    "    class_summary[\"Percent\"] = class_summary[\"Percent\"].round(2)\n",
    "    print(class_summary)\n",
    "    normalize(df)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date-Time\"]).dt.date\n",
    "    df.groupby([\"Date\"]).size()\n",
    "    df_train = df[(df[\"Date\"] >= date(2025, 1, 3)) & (df[\"Date\"] <= date(2025, 1, 10))]\n",
    "    df_val = df[(df[\"Date\"] >= date(2025, 1, 25)) & (df[\"Date\"] <= date(2025, 1, 27))]\n",
    "    df_test = df[(df[\"Date\"] >= date(2025, 1, 28)) & (df[\"Date\"] <= date(2025, 1, 31))]\n",
    "    target_cols = [f\"Target_{i}\" for i in [10, 50, 100]]\n",
    "    price_cols = [f\"L{i}-BidPrice\" for i in range(1, 11)] + [f\"L{i}-AskPrice\" for i in range(1, 11)]\n",
    "    size_cols  = [f\"L{i}-BidSize\"  for i in range(1, 11)] + [f\"L{i}-AskSize\"  for i in range(1, 11)]\n",
    "    df_train = df_train[price_cols + size_cols+target_cols]\n",
    "    df_test =  df_test[price_cols + size_cols+target_cols]\n",
    "    df_val = df_val[price_cols + size_cols+target_cols]\n",
    "    if None in (dataset_train, dataset_val, dataset_test):\n",
    "        dataset_train = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "    else:\n",
    "        dataset_train1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_train.merge(dataset_train1)\n",
    "        dataset_val.merge(dataset_val1)\n",
    "        dataset_test.merge(dataset_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d6f56-3c76-4f31-82bc-5bb3ae4f6959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:19:52.531388Z",
     "start_time": "2025-11-28T02:19:32.048113Z"
    }
   },
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    log.info(\"Processing {ticker}\")\n",
    "    dataset_train: Optional[Dataset] = None\n",
    "    dataset_val: Optional[Dataset] = None\n",
    "    dataset_test: Optional[Dataset] = None\n",
    "    df = pd.read_csv(f\"{data_path}{ticker}{prefix}.csv\",engine=\"pyarrow\",sep = ',')\n",
    "    df.head()\n",
    "    df[\"Date-Time\"] = pd.to_datetime(df[\"Date-Time\"])\n",
    "    df[\"Date-Time\"] = df[\"Date-Time\"].dt.tz_convert(\"America/New_York\")\n",
    "    add_horizons(df,(10, 50, 100),alpha)\n",
    "    class_summary = df.groupby(\"Target_100\").size().reset_index(name=\"Count\")\n",
    "    # Calculate relative percentage\n",
    "    class_summary[\"Percent\"] = (class_summary[\"Count\"] / class_summary[\"Count\"].sum()) * 100\n",
    "    class_summary[\"Percent\"] = class_summary[\"Percent\"].round(2)\n",
    "    class_summary\n",
    "    normalize(df)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date-Time\"]).dt.date\n",
    "    df.groupby([\"Date\"]).size()\n",
    "    df_train = df[(df[\"Date\"] >= date(2025, 1, 3)) & (df[\"Date\"] <= date(2025, 1, 10))]\n",
    "    df_val = df[(df[\"Date\"] >= date(2025, 1, 25)) & (df[\"Date\"] <= date(2025, 1, 27))]\n",
    "    df_test = df[(df[\"Date\"] >= date(2025, 1, 28)) & (df[\"Date\"] <= date(2025, 1, 31))]\n",
    "    target_cols = [f\"Target_{i}\" for i in [10, 50, 100]]\n",
    "    price_cols = [f\"L{i}-BidPrice\" for i in range(1, 11)] + [f\"L{i}-AskPrice\" for i in range(1, 11)]\n",
    "    size_cols  = [f\"L{i}-BidSize\"  for i in range(1, 11)] + [f\"L{i}-AskSize\"  for i in range(1, 11)]\n",
    "    df_train = df_train[price_cols + size_cols+target_cols]\n",
    "    df_test =  df_test[price_cols + size_cols+target_cols]\n",
    "    df_val = df_val[price_cols + size_cols+target_cols]\n",
    "    if None in (dataset_train, dataset_val, dataset_test):\n",
    "        dataset_train = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "    else:\n",
    "        dataset_train1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_train.merge(dataset_train1)\n",
    "        dataset_val.merge(dataset_val1)\n",
    "        dataset_test.merge(dataset_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd0829ba28ef7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:20:28.598415Z",
     "start_time": "2025-11-28T02:20:28.576668Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21b1cde6b92ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:20:30.812205Z",
     "start_time": "2025-11-28T02:20:30.810051Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f042eb1654ae3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:20:36.394379Z",
     "start_time": "2025-11-28T02:20:36.043292Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.deeplob import deeplob as deeplob\n",
    "model = deeplob(device=device,y_len = dataset_train.num_classes)\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_deeplob.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "from models.gd import GradientDescent as GradientDescent\n",
    "train_losses, val_losses = GradientDescent(device).batch(model, criterion, optimizer,\n",
    "                                    train_loader, val_loader,model_savepoint, epochs=50)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec751a197d584a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:41:22.648392Z",
     "start_time": "2025-11-28T02:36:41.963971Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.cnn1 import CNN1\n",
    "\n",
    "model = CNN1(num_classes = dataset_train.num_classes)\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_cnn1.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_losses, val_losses = GradientDescent(device).batch(model, criterion, optimizer,\n",
    "                                    train_loader, val_loader,model_savepoint, epochs=50)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7388c027a007f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T02:41:22.815341Z",
     "start_time": "2025-11-28T02:41:22.814056Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.mlp import MLP\n",
    "\n",
    "model = MLP()\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_mlp.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_losses, val_losses = GradientDescent(device).batch(model, criterion, optimizer,\n",
    "                                    train_loader, val_loader,model_savepoint, epochs=50)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
