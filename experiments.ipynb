{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load packages\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)\n",
    "# N, D = X_train.shape\n",
    "from datetime import date\n",
    "import logging as log\n",
    "from models.dataset import Dataset as Dataset\n",
    "\n",
    "from models.utils import add_horizons, normalize,normalize_apply,normalize_train,ad_normalize,normalize_by_prev_day,print_cfm\n",
    "from models.gd import GradientDescent as GradientDescent\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "584bebf9b147efcb",
   "metadata": {},
   "source": [
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)                # for GPU\n",
    "    torch.cuda.manual_seed_all(seed)            # for multi-GPU\n",
    "\n",
    "    # Make cuDNN deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b26b79b5b1e8a110",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# parameters\n",
    "tickers=\"slv\"\n",
    "#tickers=\"slv,\"\n",
    "#tickers=(\"slv\",)\n",
    "iteration_no=200\n",
    "model_shortname=\"all\"\n",
    "#0-> 10, 1->50,2->100\n",
    "horizon_mapping = {\n",
    "    0: 10,\n",
    "    1: 50,\n",
    "    2: 100\n",
    "}\n",
    "horizon_to_predict=2\n",
    "look_back_window=100\n",
    "batch_size=64\n",
    "alpha=0.01"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95f4bb19e5d9368b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "model_checkpoint_loc=f\"/results1/iteration{iteration_no}\"\n",
    "logs_loc=f\"results1/iteration{iteration_no}\"\n",
    "os.makedirs(f\".{model_checkpoint_loc}\", exist_ok=True)\n",
    "tickers = tuple(x.strip() for x in tickers.split(\",\"))\n",
    "print(f\"Running model with params Ticker: {tickers} Horizon Idx:{horizon_to_predict} window:{look_back_window} training batch size:{batch_size} alpha {alpha} iteration {iteration_no}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0039254e-6772-48e5-b1ea-8d4efb925853",
   "metadata": {},
   "source": [
    "# Configure basic logging to a file\n",
    "log.basicConfig(\n",
    "    filename=f\"{logs_loc}/experiments.log\",  # Name of the log file\n",
    "    level=log.INFO,             # Minimum logging level to capture (e.g., INFO, DEBUG, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format of the log messages\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'      # Format for the timestamp\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61849c91-7981-40fe-a8f8-60f4a6d0270b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#data_path=\"data/\"\n",
    "data_path=\"/home/nnagarajan/github/DNNLOB-FA800/data/etf/jan2025/cleaned/\"\n",
    "prefix=\"_cleaned_jan2025\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fcc3069812eee63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "for ticker in tickers:\n",
    "    log.info(f\"Processing {ticker}\")\n",
    "    print(f\"Processing {ticker}\")\n",
    "    dataset_train: Optional[Dataset] = None\n",
    "    dataset_val: Optional[Dataset] = None\n",
    "    dataset_test: Optional[Dataset] = None\n",
    "    df = pd.read_csv(f\"{data_path}{ticker}{prefix}.csv\",engine=\"pyarrow\",sep = ',')\n",
    "    print(df.describe())\n",
    "    df[\"Date-Time\"] = pd.to_datetime(df[\"Date-Time\"])\n",
    "    df[\"Date-Time\"] = df[\"Date-Time\"].dt.tz_convert(\"America/New_York\")\n",
    "    df=add_horizons(df,(10, 50, 100),alpha)\n",
    "    class_summary = df.groupby(f\"Target_{horizon_mapping[horizon_to_predict]}\").size().reset_index(name=\"Count\")\n",
    "    # Calculate relative percentage\n",
    "    class_summary[\"Percent\"] = (class_summary[\"Count\"] / class_summary[\"Count\"].sum()) * 100\n",
    "    class_summary[\"Percent\"] = class_summary[\"Percent\"].round(2)\n",
    "    print(class_summary)\n",
    "    #df=ad_normalize(df,100)\n",
    "    normalize(df)\n",
    "    #normalize_by_prev_day(df)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date-Time\"]).dt.date\n",
    "    df.groupby([\"Date\"]).size()\n",
    "    df_train = df[(df[\"Date\"] >= date(2025, 1, 3)) & (df[\"Date\"] <= date(2025, 1, 24))]\n",
    "    df_val = df[(df[\"Date\"] >= date(2025, 1, 25)) & (df[\"Date\"] <= date(2025, 1, 29))]\n",
    "    df_test = df[(df[\"Date\"] >= date(2025, 1, 30)) & (df[\"Date\"] <= date(2025, 1, 31))]\n",
    "    #df_train, scaler = normalize_train(df_train)\n",
    "    #df_val = normalize_apply(df_val,scaler)\n",
    "    #df_test = normalize_apply(df_test,scaler)\n",
    "    target_cols = [f\"Target_{i}\" for i in [10, 50, 100]]\n",
    "    price_cols = [f\"L{i}-BidPrice\" for i in range(1, 11)] + [f\"L{i}-AskPrice\" for i in range(1, 11)]\n",
    "    size_cols  = [f\"L{i}-BidSize\"  for i in range(1, 11)] + [f\"L{i}-AskSize\"  for i in range(1, 11)]\n",
    "    df_train = df_train[price_cols + size_cols +target_cols]\n",
    "    df_val = df_val[price_cols + size_cols +target_cols]\n",
    "    df_test =  df_test[price_cols + size_cols +target_cols]\n",
    "    if None in (dataset_train, dataset_val, dataset_test):\n",
    "        dataset_train = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val = Dataset(data=df_val.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test = Dataset(data=df_test.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "    else:\n",
    "        dataset_train1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val1 = Dataset(data=df_val.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test1 = Dataset(data=df_test.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_train.merge(dataset_train1)\n",
    "        dataset_val.merge(dataset_val1)\n",
    "        dataset_test.merge(dataset_test1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "746291e97b2dc2f1",
   "metadata": {},
   "source": [
    "df_train.head(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55fd0829ba28ef7f",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e21b1cde6b92ac5",
   "metadata": {},
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape)\n",
    "print(dataset_val.x.shape, dataset_val.y.shape)\n",
    "print(dataset_test.x.shape, dataset_test.y.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8f042eb1654ae3f",
   "metadata": {},
   "source": [
    "from models.deeplob import deeplob as deeplob\n",
    "model = deeplob(device=device,y_len = dataset_train.num_classes)\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_deeplob.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "from models.gd import GradientDescent as GradientDescent\n",
    "train_losses, val_losses = GradientDescent(device).batch(model, criterion, optimizer,\n",
    "                                    train_loader, val_loader,model_savepoint, epochs=50)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))\n",
    "print_cfm(all_targets,all_predictions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec751a197d584a2c",
   "metadata": {},
   "source": [
    "from models.cnn1 import CNN1\n",
    "\n",
    "model = CNN1(num_classes = dataset_train.num_classes)\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_cnn1.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_losses, val_losses = GradientDescent(device).batch(model, criterion, optimizer,\n",
    "                                    train_loader, val_loader,model_savepoint, epochs=50)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))\n",
    "print_cfm(all_targets,all_predictions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89a7388c027a007f",
   "metadata": {},
   "source": [
    "from models.mlp import MLP\n",
    "\n",
    "model = MLP()\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_mlp.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_losses, val_losses = GradientDescent(device).batch(model, criterion, optimizer,\n",
    "                                    train_loader, val_loader,model_savepoint, epochs=50)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))\n",
    "print_cfm(all_targets,all_predictions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "362ca42c-110e-4d78-9086-1203ab595266",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
