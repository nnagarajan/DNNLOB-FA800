{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:56:31.297000Z",
     "iopub.status.busy": "2025-11-28T04:56:31.296160Z",
     "iopub.status.idle": "2025-11-28T04:56:35.235145Z",
     "shell.execute_reply": "2025-11-28T04:56:35.231221Z",
     "shell.execute_reply.started": "2025-11-28T04:56:31.296938Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-11-30T05:11:48.278308Z",
     "start_time": "2025-11-30T05:11:48.276155Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load packages\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)\n",
    "# N, D = X_train.shape\n",
    "from datetime import date\n",
    "import logging as log\n",
    "from models.dataset import Dataset as Dataset\n",
    "\n",
    "from models.utils import add_horizons, normalize\n",
    "from models.gd import GradientDescent as GradientDescent\n"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "b26b79b5b1e8a110",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:57:59.915434Z",
     "iopub.status.busy": "2025-11-28T04:57:59.914572Z",
     "iopub.status.idle": "2025-11-28T04:57:59.928623Z",
     "shell.execute_reply": "2025-11-28T04:57:59.925535Z",
     "shell.execute_reply.started": "2025-11-28T04:57:59.915380Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ],
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:07.128083Z",
     "start_time": "2025-11-30T05:12:07.126288Z"
    }
   },
   "source": [
    "# parameters\n",
    "tickers=\"slv\"\n",
    "#tickers=\"slv,\"\n",
    "#tickers=(\"slv\",)\n",
    "iteration_no=1\n",
    "model_shortname=\"slv\"\n",
    "#0-> 10, 1->50,2->100\n",
    "horizon_mapping = {\n",
    "    0: 10,\n",
    "    1: 50,\n",
    "    2: 100\n",
    "}\n",
    "horizon_to_predict=2\n",
    "look_back_window=100\n",
    "batch_size=2048\n",
    "alpha=0.01"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "id": "95f4bb19e5d9368b",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:58:00.370984Z",
     "iopub.status.busy": "2025-11-28T04:58:00.370102Z",
     "iopub.status.idle": "2025-11-28T04:58:00.382396Z",
     "shell.execute_reply": "2025-11-28T04:58:00.379193Z",
     "shell.execute_reply.started": "2025-11-28T04:58:00.370928Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:07.539763Z",
     "start_time": "2025-11-30T05:12:07.537467Z"
    }
   },
   "source": [
    "model_checkpoint_loc=f\"/home/nnagarajan/github/DNNLOB-FA800-GH/results/iteration{iteration_no}\"\n",
    "log_loc=f\"results/iteration{iteration_no}\"\n",
    "os.makedirs(f\".{model_checkpoint_loc}\", exist_ok=True)\n",
    "tickers = tuple(x.strip() for x in tickers.split(\",\"))\n",
    "print(f\"Running model with params Ticker: {tickers} Horizon Idx:{horizon_to_predict} window:{look_back_window} training batch size:{batch_size} alpha {alpha} iteration {iteration_no}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model with params Ticker: ('slv',) Horizon Idx:2 window:100 training batch size:2048 alpha 0.01 iteration 1\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "id": "0039254e-6772-48e5-b1ea-8d4efb925853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:07.720711Z",
     "start_time": "2025-11-30T05:12:07.719023Z"
    }
   },
   "source": [
    "# Configure basic logging to a file\n",
    "log.basicConfig(\n",
    "    filename=f\"{log_loc}/transferlearning.log\",  # Name of the log file\n",
    "    level=log.INFO,             # Minimum logging level to capture (e.g., INFO, DEBUG, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format of the log messages\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'      # Format for the timestamp\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "id": "61849c91-7981-40fe-a8f8-60f4a6d0270b",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-11-28T04:58:00.370984Z",
     "iopub.status.busy": "2025-11-28T04:58:00.370102Z",
     "iopub.status.idle": "2025-11-28T04:58:00.382396Z",
     "shell.execute_reply": "2025-11-28T04:58:00.379193Z",
     "shell.execute_reply.started": "2025-11-28T04:58:00.370928Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:07.904722Z",
     "start_time": "2025-11-30T05:12:07.903196Z"
    }
   },
   "source": [
    "data_path=\"/home/nnagarajan/github/DNNLOB-FA800/data/etf/jan2025/cleaned/\"\n",
    "prefix=\"_cleaned_jan2025\""
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "id": "9fcc3069812eee63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:08.690853Z",
     "start_time": "2025-11-30T05:12:08.077365Z"
    }
   },
   "source": [
    "for ticker in tickers:\n",
    "    log.info(f\"Processing {ticker}\")\n",
    "    print(f\"Processing {ticker}\")\n",
    "    dataset_train: Optional[Dataset] = None\n",
    "    dataset_val: Optional[Dataset] = None\n",
    "    dataset_test: Optional[Dataset] = None\n",
    "    df = pd.read_csv(f\"{data_path}{ticker}{prefix}.csv\",engine=\"pyarrow\",sep = ',')\n",
    "    print(df.describe())\n",
    "    df[\"Date-Time\"] = pd.to_datetime(df[\"Date-Time\"])\n",
    "    df[\"Date-Time\"] = df[\"Date-Time\"].dt.tz_convert(\"America/New_York\")\n",
    "    df=add_horizons(df,(10, 50, 100),alpha)\n",
    "    class_summary = df.groupby(f\"Target_{horizon_mapping[horizon_to_predict]}\").size().reset_index(name=\"Count\")\n",
    "    # Calculate relative percentage\n",
    "    class_summary[\"Percent\"] = (class_summary[\"Count\"] / class_summary[\"Count\"].sum()) * 100\n",
    "    class_summary[\"Percent\"] = class_summary[\"Percent\"].round(2)\n",
    "    print(class_summary)\n",
    "    #df=ad_normalize(df,100)\n",
    "    normalize(df)\n",
    "    #normalize_by_prev_day(df)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date-Time\"]).dt.date\n",
    "    df.groupby([\"Date\"]).size()\n",
    "    df_train = df[(df[\"Date\"] >= date(2025, 1, 3)) & (df[\"Date\"] <= date(2025, 1, 24))]\n",
    "    df_val = df[(df[\"Date\"] >= date(2025, 1, 25)) & (df[\"Date\"] <= date(2025, 1, 27))]\n",
    "    df_test = df[(df[\"Date\"] >= date(2025, 1, 28)) & (df[\"Date\"] <= date(2025, 1, 31))]\n",
    "    #df_train, scaler = normalize_train(df_train)\n",
    "    #df_val = normalize_apply(df_val,scaler)\n",
    "    #df_test = normalize_apply(df_test,scaler)\n",
    "    target_cols = [f\"Target_{i}\" for i in [10, 50, 100]]\n",
    "    price_cols = [f\"L{i}-BidPrice\" for i in range(1, 11)] + [f\"L{i}-AskPrice\" for i in range(1, 11)]\n",
    "    size_cols  = [f\"L{i}-BidSize\"  for i in range(1, 11)] + [f\"L{i}-AskSize\"  for i in range(1, 11)]\n",
    "    df_train = df_train[price_cols + size_cols +target_cols]\n",
    "    df_val = df_val[price_cols + size_cols +target_cols]\n",
    "    df_test =  df_test[price_cols + size_cols +target_cols]\n",
    "    if None in (dataset_train, dataset_val, dataset_test):\n",
    "        dataset_train = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val = Dataset(data=df_val.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test = Dataset(data=df_test.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "    else:\n",
    "        dataset_train1 = Dataset(data=df_train.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_val1 = Dataset(data=df_val.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_test1 = Dataset(data=df_test.to_numpy(), k=horizon_to_predict, num_classes=3, T=look_back_window)\n",
    "        dataset_train.merge(dataset_train1)\n",
    "        dataset_val.merge(dataset_val1)\n",
    "        dataset_test.merge(dataset_test1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing slv\n",
      "   Target_100  Count  Percent\n",
      "0           0  23270    21.90\n",
      "1           1  40368    37.98\n",
      "2           2  42636    40.12\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "id": "55fd0829ba28ef7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:09.448913Z",
     "start_time": "2025-11-30T05:12:09.447347Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "id": "9e21b1cde6b92ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:09.577367Z",
     "start_time": "2025-11-30T05:12:09.571311Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31806, 1, 100, 40]) torch.Size([31806])\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "id": "e8f042eb1654ae3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:11.028306Z",
     "start_time": "2025-11-30T05:12:10.220066Z"
    }
   },
   "source": [
    "from models.deeplob import deeplob as deeplob\n",
    "model = deeplob(device=device,y_len = dataset_train.num_classes)\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_deeplob.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "from models.gd import GradientDescent as GradientDescent\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.3808715336728919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2384    0.0705    0.1089      7528\n",
      "           1     0.3965    0.5193    0.4497     12302\n",
      "           2     0.3857    0.4337    0.4083     11976\n",
      "\n",
      "    accuracy                         0.3809     31806\n",
      "   macro avg     0.3402    0.3412    0.3223     31806\n",
      "weighted avg     0.3550    0.3809    0.3534     31806\n",
      "\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "id": "ec751a197d584a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:48.361657Z",
     "start_time": "2025-11-30T05:12:47.948101Z"
    }
   },
   "source": [
    "from models.cnn1 import CNN1\n",
    "\n",
    "model = CNN1(num_classes = dataset_train.num_classes)\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_cnn1.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.4042319059296988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1356    0.0011    0.0021      7528\n",
      "           1     0.3987    0.7462    0.5198     12302\n",
      "           2     0.4205    0.3064    0.3545     11976\n",
      "\n",
      "    accuracy                         0.4042     31806\n",
      "   macro avg     0.3183    0.3512    0.2921     31806\n",
      "weighted avg     0.3447    0.4042    0.3350     31806\n",
      "\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "id": "89a7388c027a007f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:12:49.831393Z",
     "start_time": "2025-11-30T05:12:49.298525Z"
    }
   },
   "source": [
    "from models.mlp import MLP\n",
    "\n",
    "model = MLP()\n",
    "model.to(device)\n",
    "model_savepoint=f\"{model_checkpoint_loc}/best_val_model_{model_shortname}_mlp.pt\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "all_targets, all_predictions = GradientDescent(device).evaulate_model(model_savepoint, model, test_loader)\n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.3652769917625605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2359    0.2105    0.2225      7528\n",
      "           1     0.4179    0.4414    0.4293     12302\n",
      "           2     0.3806    0.3844    0.3825     11976\n",
      "\n",
      "    accuracy                         0.3653     31806\n",
      "   macro avg     0.3448    0.3454    0.3448     31806\n",
      "weighted avg     0.3608    0.3653    0.3627     31806\n",
      "\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d75e4b137aa1bee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
